#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ø–Ω–¥–µ–∫—Å.–í–æ—Ä–¥—Å—Ç–∞—Ç
"""

import os
import sys
import json
import re
from datetime import datetime, timedelta
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__))))

from wordstat_client import WordstatClient, WordstatDataProcessor
from wordstat_config import config

STOP_TOKENS = {
    "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–∏–ª–∏", "–æ—Ç", "–¥–æ", "–ø–æ–¥", "–ø—Ä–∏", "–∑–∞", "–∫", "–∏–∑", "–æ",
    "the", "and", "for", "with", "from", "to", "of", "a", "an",
}

BEAUTY_ROOTS = (
    "—Å—Ç—Ä–∏–∂", "–≤–æ–ª–æ—Å", "–æ–∫—Ä–∞—à", "–º–µ–ª–∏—Ä", "–±—Ä–æ–≤", "—Ä–µ—Å–Ω–∏—Ü", "–º–∞–Ω–∏–∫", "–ø–µ–¥–∏–∫",
    "–Ω–æ–≥—Ç", "–∫–æ—Å–º–µ—Ç", "–ø–∏–ª–∏–Ω–≥", "–ª–∏—Ñ—Ç", "–±–æ—Ç–æ–∫—Å", "–º–∞—Å—Å–∞–∂", "—Å–ø–∞", "—ç–ø–∏–ª—è",
    "–æ–º–æ–ª–æ–∂", "—É—Ö–æ–¥", "–ø–∞—Ä–∏–∫–º–∞—Ö", "—Å–∞–ª–æ–Ω",
)

def _extract_tokens(text: str):
    raw = re.findall(r"[a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9-]+", (text or "").lower())
    out = []
    for t in raw:
        if len(t) < 3 or t in STOP_TOKENS:
            continue
        out.append(t)
    return out

def _build_relevance_terms(beauty_keywords, cursor):
    terms = set()
    for phrase in beauty_keywords:
        terms.update(_extract_tokens(phrase))

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ —É—Å–ª—É–≥, —á—Ç–æ–±—ã —Ñ–∏–ª—å—Ç—Ä –ø–æ–¥—Ö–≤–∞—Ç—ã–≤–∞–ª —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å –±–∏–∑–Ω–µ—Å–∞.
    try:
        cursor.execute(
            """
            SELECT name, description
            FROM userservices
            WHERE (is_active IS TRUE OR is_active IS NULL)
            ORDER BY updated_at DESC NULLS LAST
            LIMIT 5000
            """
        )
        for row in cursor.fetchall() or []:
            if hasattr(row, "keys"):
                name = row.get("name") or ""
                desc = row.get("description") or ""
            else:
                name = row[0] if len(row) > 0 else ""
                desc = row[1] if len(row) > 1 else ""
            terms.update(_extract_tokens(name))
            terms.update(_extract_tokens(desc))
    except Exception:
        pass

    return terms

def _load_city_terms(cursor):
    cities = set()
    try:
        cursor.execute(
            """
            SELECT DISTINCT city
            FROM businesses
            WHERE city IS NOT NULL AND btrim(city) <> ''
            LIMIT 500
            """
        )
        for row in cursor.fetchall() or []:
            city = (row[0] if not hasattr(row, "keys") else row.get("city") or "").strip().lower()
            if len(city) >= 3:
                cities.add(city)
    except Exception:
        pass
    return cities

def _is_noise_keyword(keyword: str) -> bool:
    q = (keyword or "").strip().lower()
    if len(q) < 3:
        return True
    # –®—É–º —Ç–∏–ø–∞ "a an", "c a", "x y z"
    if re.fullmatch(r"[a-z]{1,2}(?:\s+[a-z]{1,2}){0,4}", q):
        return True
    tokens = _extract_tokens(q)
    if not tokens:
        return True
    # –ù–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –Ω–µ—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –∏ –Ω–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã—Ö –∫–æ—Ä–Ω–µ–π.
    has_cyr = bool(re.search(r"[–∞-—è—ë]", q))
    if not has_cyr and not any(root in q for root in BEAUTY_ROOTS):
        return True
    return False

def _is_relevant_keyword(keyword: str, relevance_terms, city_terms) -> bool:
    q = (keyword or "").strip().lower()
    if _is_noise_keyword(q):
        return False
    if any(term in q for term in relevance_terms):
        return True
    # –î–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å –≥–æ—Ä–æ–¥–æ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –±—å—é—Ç–∏-–∫–æ—Ä–µ–Ω—å.
    if any(city in q for city in city_terms) and any(root in q for root in BEAUTY_ROOTS):
        return True
    return any(root in q for root in BEAUTY_ROOTS)

def _extract_queries(api_payload):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –æ—Ç–≤–µ—Ç—ã Wordstat topRequests –≤ —Å–ø–∏—Å–æ–∫ {key, clicks}.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç-–æ–±—ä–µ–∫—Ç –∏ –º–∞—Å—Å–∏–≤ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Ñ—Ä–∞–∑–∞–º.
    """
    if not api_payload:
        return []

    blocks = api_payload if isinstance(api_payload, list) else [api_payload]
    rows = []

    for block in blocks:
        if not isinstance(block, dict):
            continue

        for section in ("topRequests", "top_requests", "associations", "alsoSearch"):
            items = block.get(section) or []
            if not isinstance(items, list):
                continue
            for item in items:
                if not isinstance(item, dict):
                    continue
                text = (
                    item.get("text")
                    or item.get("phrase")
                    or item.get("query")
                    or item.get("key")
                    or ""
                ).strip()
                if not text:
                    continue
                count = (
                    item.get("count")
                    or item.get("shows")
                    or item.get("clicks")
                    or 0
                )
                try:
                    count = int(count)
                except (TypeError, ValueError):
                    count = 0
                rows.append({"key": text, "clicks": count})

    # –¥–µ–¥—É–ø: –æ—Å—Ç–∞–≤–ª—è–µ–º –º–∞–∫—Å–∏–º—É–º –ø–æ –ø–æ–∫–∞–∑–∞–º
    by_key = {}
    for r in rows:
        key = r["key"].lower().strip()
        prev = by_key.get(key)
        if not prev or r["clicks"] > prev["clicks"]:
            by_key[key] = r
    return list(by_key.values())

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    
    print("üîÑ –ó–∞–ø—É—Å–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –Ø–Ω–¥–µ–∫—Å.–í–æ—Ä–¥—Å—Ç–∞—Ç...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    if not config.is_configured():
        print("‚ùå API –Ø–Ω–¥–µ–∫—Å.–í–æ—Ä–¥—Å—Ç–∞—Ç –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        print(f"üîó –ü–æ–ª—É—á–∏—Ç–µ OAuth —Ç–æ–∫–µ–Ω –ø–æ —Å—Å—ã–ª–∫–µ: {config.get_auth_url()}")
        print("üìù –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ç–æ–∫–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è YANDEX_WORDSTAT_OAUTH_TOKEN")
        return False
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç
    client = WordstatClient(config.client_id, config.client_secret)
    client.set_access_token(config.oauth_token)
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±—å—é—Ç–∏-–∏–Ω–¥—É—Å—Ç—Ä–∏–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω—ã, –≤–∫–ª—é—á–∞—è –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—é)
    beauty_keywords = [
        # –í–æ–ª–æ—Å—ã/—Å—Ç—Ä–∏–∂–∫–∏/–æ–∫—Ä–∞—à–∏–≤–∞–Ω–∏–µ
        "—Å—Ç—Ä–∏–∂–∫–∞ –∂–µ–Ω—Å–∫–∞—è", "—Å—Ç—Ä–∏–∂–∫–∞ –º—É–∂—Å–∫–∞—è", "—É–∫–ª–∞–¥–∫–∞ –≤–æ–ª–æ—Å",
        "–æ–∫—Ä–∞—à–∏–≤–∞–Ω–∏–µ –≤–æ–ª–æ—Å", "–º–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "–±–ª–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ",
        "–ø–∞—Ä–∏–∫–º–∞—Ö–µ—Ä—Å–∫–∞—è", "—Å–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã", "–±–∞—Ä–±–µ—Ä—à–æ–ø",
        # –ù–æ–≥—Ç–∏
        "–º–∞–Ω–∏–∫—é—Ä", "–ø–µ–¥–∏–∫—é—Ä", "–≥–µ–ª—å-–ª–∞–∫", "–Ω–∞—Ä–∞—â–∏–≤–∞–Ω–∏–µ –Ω–æ–≥—Ç–µ–π",
        # SPA/–º–∞—Å—Å–∞–∂
        "–º–∞—Å—Å–∞–∂", "—Å–ø–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã", "–æ–±–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ",
        # –ë—Ä–æ–≤–∏/—Ä–µ—Å–Ω–∏—Ü—ã
        "–±—Ä–æ–≤–∏", "—Ä–µ—Å–Ω–∏—Ü—ã", "–ª–∞–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –±—Ä–æ–≤–µ–π", "–ª–∞–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—Å–Ω–∏—Ü",
        # –ö–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—è ‚Äî –¥–æ–±–∞–≤–ª–µ–Ω–æ
        "–∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—è", "–∫–æ—Å–º–µ—Ç–æ–ª–æ–≥", "—á–∏—Å—Ç–∫–∞ –ª–∏—Ü–∞", "–ø–∏–ª–∏–Ω–≥ –ª–∏—Ü–∞",
        "–±–æ—Ç–æ–∫—Å", "–¥–∏—Å–ø–æ—Ä—Ç", "–∫–æ–Ω—Ç—É—Ä–Ω–∞—è –ø–ª–∞—Å—Ç–∏–∫–∞", "—Ñ–∏–ª–ª–µ—Ä—ã",
        "–≥–∏–∞–ª—É—Ä–æ–Ω–æ–≤–∞—è –∫–∏—Å–ª–æ—Ç–∞", "–±–∏–æ—Ä–µ–≤–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è", "–º–µ–∑–æ—Ç–µ—Ä–∞–ø–∏—è",
        "–ø–ª–∞–∑–º–æ–ª–∏—Ñ—Ç–∏–Ω–≥", "RF-–ª–∏—Ñ—Ç–∏–Ω–≥", "SMAS-–ª–∏—Ñ—Ç–∏–Ω–≥", "—É–ª—å—Ç—Ä–∞–∑–≤—É–∫–æ–≤–æ–π SMAS",
        "–ª–∞–∑–µ—Ä–Ω–∞—è —ç–ø–∏–ª—è—Ü–∏—è", "—Ñ–æ—Ç–æ—ç–ø–∏–ª—è—Ü–∏—è", "–ª–∞–∑–µ—Ä–Ω–æ–µ –æ–º–æ–ª–æ–∂–µ–Ω–∏–µ",
        "–ª–∞–∑–µ—Ä–Ω–∞—è —à–ª–∏—Ñ–æ–≤–∫–∞", "–Ω–∏—Ç–µ–≤–æ–π –ª–∏—Ñ—Ç–∏–Ω–≥", "–ª–∏–ø–æ–ª–∏—Ç–∏–∫–∏",
        "–º–∏–∫—Ä–æ—Ç–æ–∫–∏", "–∞–ø–ø–∞—Ä–∞—Ç–Ω–∞—è –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—è", "–¥–µ—Ä–º–∞–ø–µ–Ω", "–º–∏–∫—Ä–æ–Ω–∏–¥–ª–∏–Ω–≥",
        "–∞–Ω—Ç–∏–≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã", "–ª–µ—á–µ–Ω–∏–µ –∞–∫–Ω–µ", "–ø–æ—Å—Ç–∞–∫–Ω–µ", "–∫—É–ø–∏—Ç—å –∫—É–ø–µ—Ä–æ–∑",
        "—É—Ö–æ–¥ –∑–∞ –∫–æ–∂–µ–π", "–æ–º–æ–ª–æ–∂–µ–Ω–∏–µ –ª–∏—Ü–∞", "–º–∞—Å–∫–∞ –¥–ª—è –ª–∏—Ü–∞"
    ]
    
    print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(beauty_keywords)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        print("üìä –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
        popular_data = client.get_popular_queries(beauty_keywords, config.default_region)
        
        if not popular_data:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ—Ç API")
            return False

        all_queries = _extract_queries(popular_data)
        if not all_queries:
            print("‚ùå –í –æ—Ç–≤–µ—Ç–µ API –Ω–µ—Ç topRequests/associations")
            return False
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ë–î
        from database_manager import DatabaseManager
        from service_categorizer import categorizer
        import uuid
        
        db = DatabaseManager()
        cursor = db.conn.cursor()
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS wordstatkeywords (
                id TEXT PRIMARY KEY,
                keyword TEXT UNIQUE NOT NULL,
                views INTEGER DEFAULT 0,
                category TEXT DEFAULT 'other',
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
        )
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_wordstat_views ON wordstatkeywords(views DESC)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_wordstat_category ON wordstatkeywords(category)")
        db.conn.commit()
        
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü—É WordstatKeywords...")
        relevance_terms = _build_relevance_terms(beauty_keywords, cursor)
        city_terms = _load_city_terms(cursor)
        
        saved_count = 0
        updated_count = 0
        skipped_noise_count = 0
        
        try:
            for item in all_queries:
                keyword = item.get('key', '').strip()
                if not keyword:
                    continue
                if not _is_relevant_keyword(keyword, relevance_terms, city_terms):
                    skipped_noise_count += 1
                    continue
                    
                views = int(item.get('clicks', 0))
                
                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º categorizer.categorize_service, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                # –û–Ω –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (category_key, confidence, matched_keywords)
                category, confidence, _ = categorizer.categorize_service(keyword)
                
                if confidence < 0.3:
                    category = 'other'

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
                cursor.execute("SELECT id FROM wordstatkeywords WHERE keyword = %s", (keyword,))
                existing = cursor.fetchone()
                
                if existing:
                    existing_id = existing[0] if not hasattr(existing, "keys") else existing.get("id")
                    cursor.execute("""
                        UPDATE wordstatkeywords 
                        SET views = %s, category = %s, updated_at = CURRENT_TIMESTAMP 
                        WHERE id = %s
                    """, (views, category, existing_id))
                    updated_count += 1
                else:
                    new_id = str(uuid.uuid4())
                    cursor.execute("""
                        INSERT INTO wordstatkeywords (id, keyword, views, category, updated_at) 
                        VALUES (%s, %s, %s, %s, CURRENT_TIMESTAMP)
                    """, (new_id, keyword, views, category))
                    saved_count += 1
            
            db.conn.commit()
            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–î")
            print(f"   ‚ûï –ù–æ–≤—ã—Ö: {saved_count}")
            print(f"   üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_count}")
            print(f"   üßπ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —à—É–º–Ω—ã—Ö: {skipped_noise_count}")
            
        except Exception as db_err:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –ë–î: {db_err}")
            db.conn.rollback()
            return False
            
        finally:
            db.close()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–≤—Å–µ –µ—â–µ –ø–æ–ª–µ–∑–Ω–æ)
        metadata = {
            'last_update': datetime.now().isoformat(),
            'queries_count': saved_count + updated_count,
            'region': config.default_region,
            'region_name': config.get_region_name(config.default_region)
        }
        
        prompts_dir = Path(__file__).parent.parent / "prompts"
        if not prompts_dir.exists():
            prompts_dir.mkdir(parents=True, exist_ok=True)
            
        metadata_path = prompts_dir / "wordstat_metadata.json"
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {metadata_path}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_update_needed() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
    metadata_path = Path(__file__).parent.parent / "prompts" / "wordstat_metadata.json"
    
    if not metadata_path.exists():
        return True
    
    try:
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        last_update = datetime.fromisoformat(metadata['last_update'])
        update_interval = timedelta(seconds=config.update_interval)
        
        return datetime.now() - last_update > update_interval
        
    except Exception:
        return True

if __name__ == "__main__":
    if check_update_needed():
        success = main()
        if success:
            print("üéâ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        else:
            print("üí• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
            sys.exit(1)
    else:
        print("‚è∞ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (–¥–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã)")
