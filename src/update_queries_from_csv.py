#!/usr/bin/env python3
"""
–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ CSV —Ñ–∞–π–ª–æ–≤ Wordstat
"""

import os
import sys
import csv
import json
from datetime import datetime
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__))))

def load_csv_data(csv_file_path: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    data = []
    try:
        with open(csv_file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f, delimiter=';')
            for row in reader:
                if '–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞' in row and '–ß–∏—Å–ª–æ –∑–∞–ø—Ä–æ—Å–æ–≤' in row:
                    query = row['–§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞'].strip()
                    shows = int(row['–ß–∏—Å–ª–æ –∑–∞–ø—Ä–æ—Å–æ–≤'].replace(',', '')) if row['–ß–∏—Å–ª–æ –∑–∞–ø—Ä–æ—Å–æ–≤'] else 0
                    if query and shows > 0:
                        data.append({
                            'query': query,
                            'shows': shows
                        })
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {csv_file_path}: {e}")
    return data

def categorize_queries(queries_data: list) -> dict:
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–∏–ø–∞–º —É—Å–ª—É–≥"""
    categories = {
        '–°—Ç—Ä–∏–∂–∫–∏ –∏ —É–∫–ª–∞–¥–∫–∏': [],
        '–û–∫—Ä–∞—à–∏–≤–∞–Ω–∏–µ': [],
        '–ú–∞–Ω–∏–∫—é—Ä –∏ –ø–µ–¥–∏–∫—é—Ä': [],
        '–ú–∞—Å—Å–∞–∂ –∏ –°–ü–ê': [],
        '–ë—Ä–æ–≤–∏ –∏ —Ä–µ—Å–Ω–∏—Ü—ã': [],
        '–ë–∞—Ä–±–µ—Ä—à–æ–ø': [],
        '–î—Ä—É–≥–∏–µ —É—Å–ª—É–≥–∏': []
    }
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
    category_keywords = {
        '–°—Ç—Ä–∏–∂–∫–∏ –∏ —É–∫–ª–∞–¥–∫–∏': ['—Å—Ç—Ä–∏–∂–∫–∞', '—É–∫–ª–∞–¥–∫–∞', '–ø—Ä–∏—á–µ—Å–∫–∞', '–≤–æ–ª–æ—Å—ã', '–ø–∞—Ä–∏–∫–º–∞—Ö–µ—Ä', '–±–æ–±', '–∫–∞—Ä–µ', '–ø–∏–∫—Å–∏'],
        '–û–∫—Ä–∞—à–∏–≤–∞–Ω–∏–µ': ['–æ–∫—Ä–∞—à–∏–≤–∞–Ω–∏–µ', '–ø–æ–∫—Ä–∞—Å–∫–∞', '–º–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∫–æ–ª–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ç–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '–±–ª–æ–Ω–¥–∏—Ä–æ–≤–∞–Ω–∏–µ', '–±–∞–ª–∞—è–∂', '—à–∞—Ç—É—à'],
        '–ú–∞–Ω–∏–∫—é—Ä –∏ –ø–µ–¥–∏–∫—é—Ä': ['–º–∞–Ω–∏–∫—é—Ä', '–ø–µ–¥–∏–∫—é—Ä', '–Ω–æ–≥—Ç–∏', '–≥–µ–ª—å-–ª–∞–∫', '—à–µ–ª–ª–∞–∫', '–Ω–∞—Ä–∞—â–∏–≤–∞–Ω–∏–µ', '—Ñ—Ä–µ–Ω—á'],
        '–ú–∞—Å—Å–∞–∂ –∏ –°–ü–ê': ['–º–∞—Å—Å–∞–∂', '—Å–ø–∞', '–æ–±–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ', '–ø–∏–ª–∏–Ω–≥', '–∞–Ω—Ç–∏—Ü–µ–ª–ª—é–ª–∏—Ç–Ω—ã–π', '—Ä–µ–ª–∞–∫—Å', '–∞—Ä–æ–º–∞—Ç–µ—Ä–∞–ø–∏—è'],
        '–ë—Ä–æ–≤–∏ –∏ —Ä–µ—Å–Ω–∏—Ü—ã': ['–±—Ä–æ–≤–∏', '—Ä–µ—Å–Ω–∏—Ü—ã', '–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –±—Ä–æ–≤–µ–π', '–Ω–∞—Ä–∞—â–∏–≤–∞–Ω–∏–µ —Ä–µ—Å–Ω–∏—Ü', '–ª–∞–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ'],
        '–ë–∞—Ä–±–µ—Ä—à–æ–ø': ['–±–∞—Ä–±–µ—Ä—à–æ–ø', '–º—É–∂—Å–∫–∞—è —Å—Ç—Ä–∏–∂–∫–∞', '–±–æ—Ä–æ–¥–∞', '—É—Å—ã', '–±—Ä–∏—Ç—å–µ', '—Å—Ç—Ä–∏–∂–∫–∞ –ø–æ–¥ –º–∞—à–∏–Ω–∫—É']
    }
    
    for item in queries_data:
        query = item['query'].lower()
        shows = item['shows']
        
        categorized = False
        for category, keywords in category_keywords.items():
            if any(keyword in query for keyword in keywords):
                categories[category].append({
                    'query': item['query'],
                    'shows': shows
                })
                categorized = True
                break
        
        if not categorized:
            categories['–î—Ä—É–≥–∏–µ —É—Å–ª—É–≥–∏'].append({
                'query': item['query'],
                'shows': shows
            })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø–æ–∫–∞–∑–æ–≤
    for category in categories:
        categories[category].sort(key=lambda x: x['shows'], reverse=True)
    
    return categories

def save_queries_to_file(categorized_queries: dict, file_path: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Ñ–∞–π–ª"""
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write("# –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–∫–∞–∑–æ–≤ (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –∏–∑ CSV)\n\n")
        
        for category, queries in categorized_queries.items():
            if queries:
                f.write(f"## {category}:\n")
                for item in queries[:50]:  # –¢–æ–ø 50 –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    f.write(f"- {item['query']} ({item['shows']:,} –ø–æ–∫–∞–∑–æ–≤/–º–µ—Å—è—Ü)\n")
                f.write("\n")
        
        f.write(f"\n*–û–±–Ω–æ–≤–ª–µ–Ω–æ: {datetime.now().strftime('%d.%m.%Y %H:%M')}*\n")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    print("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ CSV —Ñ–∞–π–ª–æ–≤...")
    
    # –ü—É—Ç–∏ –∫ CSV —Ñ–∞–π–ª–∞–º (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ Downloads)
    csv_files = [
        "/Users/alexdemyanov/Downloads/wordstat_top_queries (1).csv",
        "/Users/alexdemyanov/Downloads/wordstat_top_queries (2).csv",
        "/Users/alexdemyanov/Downloads/wordstat_top_queries (3).csv",
        "/Users/alexdemyanov/Downloads/wordstat_top_queries (4).csv",
        "/Users/alexdemyanov/Downloads/wordstat_top_queries (5).csv",
        "/Users/alexdemyanov/Downloads/wordstat_top_queries (6).csv",
        "/Users/alexdemyanov/Downloads/wordstat_top_queries.csv",
        "/Users/alexdemyanov/Downloads/wordstat_similar_queries.csv"
    ]
    
    all_queries = []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö CSV —Ñ–∞–π–ª–æ–≤
    for csv_file in csv_files:
        if os.path.exists(csv_file):
            print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {os.path.basename(csv_file)}...")
            data = load_csv_data(csv_file)
            all_queries.extend(data)
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø—Ä–æ—Å–æ–≤")
        else:
            print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_file}")
    
    if not all_queries:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ CSV —Ñ–∞–π–ª–æ–≤ —Å –¥–∞–Ω–Ω—ã–º–∏")
        return False
    
    print(f"\nüìä –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique_queries = {}
    for item in all_queries:
        query = item['query']
        if query not in unique_queries or unique_queries[query]['shows'] < item['shows']:
            unique_queries[query] = item
    
    print(f"üìà –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(unique_queries)}")
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã
    print("\nüè∑Ô∏è  –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤...")
    categorized_queries = categorize_queries(list(unique_queries.values()))
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for category, queries in categorized_queries.items():
        if queries:
            total_shows = sum(q['shows'] for q in queries)
            print(f"   {category}: {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤, {total_shows:,} –ø–æ–∫–∞–∑–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    prompts_dir = Path(__file__).parent.parent / "prompts"
    file_path = prompts_dir / "popular_queries_with_clicks.txt"
    
    save_queries_to_file(categorized_queries, str(file_path))
    
    print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {file_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        'last_update': datetime.now().isoformat(),
        'total_queries': len(unique_queries),
        'categories': {cat: len(queries) for cat, queries in categorized_queries.items()},
        'source': 'CSV files'
    }
    
    metadata_path = prompts_dir / "wordstat_metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {metadata_path}")
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("\nüéâ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("\nüí• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏")
        sys.exit(1)
