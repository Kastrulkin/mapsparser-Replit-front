diff --git a/src/parser_interception.py b/src/parser_interception.py
index 17138f9..5f3ada0 100644
--- a/src/parser_interception.py
+++ b/src/parser_interception.py
@@ -77,6 +77,112 @@ def _find_paths(obj: Any, target_keys: List[str], max_depth: int = 6, max_previe
     return {k: v for k, v in results.items() if v}
 
 
+def _pick_first(d: Any, paths: List[List[str]]) -> Any:
+    """
+    –î–æ—Å—Ç–∞—Ç—å –ø–µ—Ä–≤–æ–µ –Ω–µ–ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ –æ–¥–Ω–æ–º—É –∏–∑ –ø—É—Ç–µ–π.
+    paths: [["data", "items", "0", "title"], ["data", "name"], ...]
+    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Å–ø–∏—Å–∫–æ–≤ ("0", "1").
+    """
+    for p in paths:
+        cur = d
+        ok = True
+        for k in p:
+            if isinstance(cur, list) and k.isdigit():
+                idx = int(k)
+                if idx < 0 or idx >= len(cur):
+                    ok = False
+                    break
+                cur = cur[idx]
+            elif isinstance(cur, dict) and k in cur:
+                cur = cur[k]
+            else:
+                ok = False
+                break
+        if ok and cur is not None and cur != "":
+            return cur
+    return None
+
+
+def _is_empty(val: Any) -> bool:
+    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–¥–ª—è merge_non_empty)."""
+    if val is None:
+        return True
+    if isinstance(val, str):
+        return not val.strip()
+    if isinstance(val, (list, dict)):
+        return len(val) == 0
+    return False
+
+
+def _is_non_empty(val: Any) -> bool:
+    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–µ–ø—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (truthy –¥–ª—è —Å–∫–∞–ª—è—Ä–æ–≤, len>0 –¥–ª—è list/dict)."""
+    if val is None:
+        return False
+    if isinstance(val, str):
+        return bool(val.strip())
+    if isinstance(val, (list, dict)):
+        return len(val) > 0
+    return bool(val)
+
+
+def merge_non_empty(dst: Dict[str, Any], src: Dict[str, Any], meta_src: Optional[Dict[str, str]] = None) -> None:
+    """
+    –ú–µ—Ä–¥–∂–∏—Ç src –≤ dst –±–µ–∑ –∑–∞—Ç–∏—Ä–∞–Ω–∏—è –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–æ–ª–µ–π –ø—É—Å—Ç—ã–º–∏.
+    - –°–∫–∞–ª—è—Ä—ã: –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ src_val truthy –∏ dst –ø—É—Å—Ç–æ.
+    - –°–ø–∏—Å–∫–∏: —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ len > 0 –∏ dst –ø—É—Å—Ç–æ.
+    - Dict: —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ –∫–ª—é—á–∞–º, –Ω–µ –∑–∞—Ç–∏—Ä–∞—Ç—å –Ω–µ–ø—É—Å—Ç–æ–µ –ø—É—Å—Ç—ã–º.
+    meta_src: {field: source} ‚Äî –ø—Ä–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø–æ–ª—è –∑–∞–ø–∏—Å–∞—Ç—å meta[field+"_source"]=source.
+    """
+    if not isinstance(src, dict):
+        return
+    meta = dst.get("meta")
+    if not isinstance(meta, dict):
+        meta = {}
+        dst["meta"] = meta
+
+    for k, v in src.items():
+        if k in ("meta", "_search_debug"):
+            continue
+        if not _is_non_empty(v):
+            continue
+        current = dst.get(k)
+        if isinstance(v, dict) and not isinstance(v, list):
+            if not isinstance(current, dict):
+                dst[k] = dict(v)
+            else:
+                merge_non_empty(current, v, meta_src)
+        elif isinstance(v, list):
+            if len(v) > 0 and _is_empty(current):
+                dst[k] = v
+                if meta_src and k in meta_src:
+                    meta[k + "_source"] = meta_src[k]
+        else:
+            if _is_empty(current):
+                dst[k] = v
+                if meta_src and k in meta_src:
+                    meta[k + "_source"] = meta_src[k]
+
+
+def _normalize_title_fields(payload: Dict[str, Any]) -> None:
+    """
+    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ title_or_name –∏ title, –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—å –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫.
+    –í—ã–∑—ã–≤–∞—Ç—å –Ω–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º payload –ø–µ—Ä–µ–¥ return.
+    """
+    t = (
+        payload.get("title_or_name")
+        or payload.get("title")
+        or payload.get("name")
+        or payload.get("page_title_short")
+        or payload.get("og_title")
+    )
+    if t and isinstance(t, str) and t.strip():
+        payload.setdefault("title_or_name", t)
+        payload.setdefault("title", t)
+        ov = payload.get("overview")
+        if isinstance(ov, dict):
+            ov.setdefault("title", t)
+
+
 def _set_if_empty(result: Dict[str, Any], key: str, value: Any) -> None:
     """
     –ü–æ—Å—Ç–∞–≤–∏—Ç—å result[key] —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç–∞–º —Å–µ–π—á–∞—Å "–ø—É—Å—Ç–æ" –∏ value –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ.
@@ -206,10 +312,17 @@ class YandexMapsInterceptionParser:
     
     def __init__(self, debug_bundle_id: Optional[str] = None):
         self.api_responses: Dict[str, Any] = {}
+        self.api_responses_list: List[tuple] = []  # [(url, json_data), ...] ‚Äî –∂—É—Ä–Ω–∞–ª –±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
+        self.location_info_payload: Optional[Dict[str, Any]] = None  # –ø–µ—Ä–≤—ã–π –≤–∞–ª–∏–¥–Ω—ã–π location-info
+        self.reviews_pages: List[Dict[str, Any]] = []
+        self.products_pages: List[Dict[str, Any]] = []
+        self.search_api_payload: Optional[Dict[str, Any]] = None  # search?business_oid=...
+        self._search_api_debug: Optional[Dict[str, Any]] = None  # –¥–ª—è selected_item_debug.json
         self.org_id: Optional[str] = None
         self.debug_bundle_id: Optional[str] = debug_bundle_id
         _base = os.getenv("DEBUG_DIR", "/app/debug_data")
         self.debug_bundle_dir: Optional[str] = os.path.join(_base, debug_bundle_id) if debug_bundle_id else None
+        self._debug_raw_services_count: int = 0
         
     def extract_org_id(self, url: str) -> Optional[str]:
         """–ò–∑–≤–ª–µ—á—å org_id –∏–∑ URL –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç
@@ -249,6 +362,17 @@ class YandexMapsInterceptionParser:
         
         print(f"üìã –ò–∑–≤–ª–µ—á–µ–Ω org_id: {self.org_id}")
 
+        def _is_captcha_page(title: str) -> bool:
+            """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–ø—á–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ, —Ä—É—Å/–∞–Ω–≥–ª)."""
+            t = (title or "").lower()
+            return (
+                "–æ–π!" in t or "captcha" in t or "robot" in t
+                or "–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç" in t
+                or "are you not a robot" in t
+                or "confirm that you" in t
+                or "–∑–∞–ø—Ä–æ—Å—ã –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –≤—ã" in t
+            )
+
         # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º bundle-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ (–µ—Å–ª–∏ –µ—â—ë –Ω–µ –∑–∞–¥–∞–Ω–∞ –≤ __init__)
         if not self.debug_bundle_id:
             ts = datetime.now().strftime("%Y%m%d_%H%M%S")
@@ -273,6 +397,12 @@ class YandexMapsInterceptionParser:
 
         # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –æ—Ç–≤–µ—Ç—ã
         self.api_responses = {}
+        self.api_responses_list = []
+        self.location_info_payload = None
+        self.reviews_pages = []
+        self.products_pages = []
+        self.search_api_payload = None
+        self._search_api_debug = None
 
         # –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –æ—Ç–≤–µ—Ç—ã
         def handle_response(response):
@@ -280,8 +410,8 @@ class YandexMapsInterceptionParser:
             try:
                 url = response.url
 
-                # –ò—â–µ–º API –∑–∞–ø—Ä–æ—Å—ã –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç
-                if "yandex.ru" in url or "yandex.net" in url:
+                # –ò—â–µ–º API –∑–∞–ø—Ä–æ—Å—ã –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç (yandex.com –ø—Ä–∏ —Ä–µ–¥–∏—Ä–µ–∫—Ç–µ —Ä–µ–≥–∏–æ–Ω–∞!)
+                if any(h in url for h in ("yandex.ru", "yandex.net", "yandex.com")):
                     # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ JSON –æ—Ç–≤–µ—Ç?
                     content_type = response.headers.get("content-type", "")
                     if "application/json" in content_type or "json" in url.lower() or "ajax=1" in url:
@@ -304,12 +434,22 @@ class YandexMapsInterceptionParser:
 
                             # Check for organization data (search or location-info)
                             if json_data:
-                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç
+                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç (–ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ URL –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                                 self.api_responses[url] = {
                                     "data": json_data,
                                     "status": response.status,
                                     "headers": dict(response.headers),
                                 }
+                                self.api_responses_list.append((url, json_data))
+                                # –ê–≥—Ä–µ–≥–∞—Ç—ã: –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º, –¥–æ–ø–æ–ª–Ω—è–µ–º
+                                if "location-info" in url and self.location_info_payload is None:
+                                    self.location_info_payload = json_data
+                                if "fetchReviews" in url or ("reviews" in url.lower() and "business" in url.lower()):
+                                    self.reviews_pages.append(json_data)
+                                if "fetchGoods" in url or "prices" in url.lower() or "goods" in url.lower():
+                                    self.products_pages.append(json_data)
+                                if "search" in url and "business_oid" in url:
+                                    self.search_api_payload = json_data
                                 # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                                 if any(
                                     keyword in url
@@ -333,7 +473,7 @@ class YandexMapsInterceptionParser:
 
         page.on("response", handle_response)
 
-        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
+        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É (interception –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω –î–û goto ‚Äî –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –±—É–¥—É—Ç –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω—ã)
         print("üåê –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –ø–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º API –∑–∞–ø—Ä–æ—Å—ã...")
         try:
             main_response = page.goto(url, wait_until="domcontentloaded", timeout=30000)
@@ -346,14 +486,12 @@ class YandexMapsInterceptionParser:
             # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–ø—á—É —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º —Ä–µ—à–µ–Ω–∏—è
             for _ in range(24):  # –ñ–¥–µ–º –¥–æ 120 —Å–µ–∫—É–Ω–¥
                 try:
-                    # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–ø—á–∏
+                    # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–ø—á–∏ (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ + —Ä—É—Å/–∞–Ω–≥–ª)
                     title = page.title()
-                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Ç–µ–∫—Å—Ç –∏ –Ω–∞–ª–∏—á–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ SmartCaptcha
                     is_captcha = (
-                        "–û–π!" in title
-                        or "Captcha" in title
-                        or "Robot" in title
+                        _is_captcha_page(title)
                         or page.get_by_text("–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ, —á—Ç–æ –≤—ã –Ω–µ —Ä–æ–±–æ—Ç").is_visible()
+                        or page.get_by_text("Are you not a robot", exact=False).is_visible()
                         or page.locator(".smart-captcha").count() > 0
                         or page.locator("input[name='smart-token']").count() > 0
                     )
@@ -372,7 +510,7 @@ class YandexMapsInterceptionParser:
 
         # Double check if we are still stuck on Captcha
         title = page.title()
-        if "–û–π!" in title or "Captcha" in title or "Robot" in title or "–í—ã –Ω–µ —Ä–æ–±–æ—Ç" in title:
+        if _is_captcha_page(title):
             print(f"‚ùå –ö–∞–ø—á–∞ –Ω–µ –±—ã–ª–∞ —Ä–µ—à–µ–Ω–∞ –∑–∞ –æ—Ç–≤–µ–¥—ë–Ω–Ω–æ–µ –≤—Ä–µ–º—è. –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
             # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ—à–∏–±–∫—É, —á—Ç–æ–±—ã –≤–æ—Ä–∫–µ—Ä –∑–Ω–∞–ª –æ –∫–∞–ø—á–µ
             return {"error": "captcha_detected", "captcha_url": page.url}
@@ -394,6 +532,31 @@ class YandexMapsInterceptionParser:
         title = page.title()
         print(f"üìç –¢–µ–∫—É—â–∏–π URL: {current_url}, –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
 
+        # –†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ /prices/ ‚Äî –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ü–µ–Ω –Ω–µ—Ç address/rating/categories. –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –Ω–∞ overview.
+        if "/prices/" in current_url or current_url.rstrip("/").endswith("/prices"):
+            print("‚ö†Ô∏è –†–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ü–µ–Ω (/prices/). –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ overview –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É URL...")
+            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π URL (—Å ll= –∏–∑ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ä—Ç—ã) –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–≥–∏–æ–Ω–∞
+            overview_url = initial_url if initial_url and "/prices/" not in initial_url else url
+            if not overview_url or "/prices/" in overview_url:
+                overview_url = re.sub(r"/prices/?", "/", current_url)
+            try:
+                # –ñ–¥—ë–º org API –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –Ω–∞ overview
+                def _is_org_api_resp(r):
+                    u = r.url
+                    return any(kw in u for kw in ("org", "location-info", "organization", "business"))
+                with page.expect_response(_is_org_api_resp, timeout=20000) as resp_info:
+                    page.goto(overview_url, wait_until="domcontentloaded", timeout=15000)
+                try:
+                    resp_info.value
+                    print("‚úÖ Org API –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –Ω–∞ overview")
+                except Exception:
+                    pass
+                page.wait_for_timeout(3000)
+                current_url = page.url
+                print(f"üìç –ü–æ—Å–ª–µ –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ overview: {current_url}")
+            except Exception as e:
+                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ overview: {e}")
+
         # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∏—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
         is_business_card = False
         try:
@@ -410,7 +573,7 @@ class YandexMapsInterceptionParser:
         except Exception:
             pass
 
-        if (not is_business_card) or ("yandex.ru" in current_url and "/org/" not in current_url):
+        if (not is_business_card) or ("yandex" in current_url and "/org/" not in current_url):
             print("‚ö†Ô∏è –ù–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏! (–†–µ–¥–∏—Ä–µ–∫—Ç?). –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–π—Ç–∏ –ø–æ —Å—Å—ã–ª–∫–µ —Å–Ω–æ–≤–∞...")
 
             # Debug: Save bad page (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ bundle)
@@ -454,12 +617,32 @@ class YandexMapsInterceptionParser:
                 page.mouse.wheel(0, 1000)
                 time.sleep(random.uniform(0.5, 1.0))
 
+        def wait_for_location_info(timeout_sec: float = 5.0) -> bool:
+            """–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ location-info API (–≥–¥–µ title). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω."""
+            start = time.time()
+            while time.time() - start < timeout_sec:
+                if any("location-info" in u for u in self.api_responses):
+                    print("‚úÖ [WAIT] location-info –∑–∞–≥—Ä—É–∂–µ–Ω")
+                    return True
+                time.sleep(0.5)
+                try:
+                    page.evaluate("window.scrollBy(0, 100)")
+                except Exception:
+                    pass
+            print("‚è±Ô∏è [WAIT] –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è location-info")
+            return False
+
         extra_photos_count = 0
+        ui_counts: Dict[str, int] = {"reviews": 0, "photos": 0, "services": 0}
 
         # 1. –°–∫—Ä–æ–ª–ª–∏–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
         print("üìú –°–∫—Ä–æ–ª–ª–∏–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
         scroll_page(3)
 
+        # 1.5 –û–∂–∏–¥–∞–Ω–∏–µ location-info –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –ø–æ –≤–∫–ª–∞–¥–∫–∞–º (title –±–µ—Ä—ë—Ç—Å—è –æ—Ç—Ç—É–¥–∞)
+        if not wait_for_location_info():
+            print("‚ö†Ô∏è –ü–µ—Ä–µ—Ö–æ–¥ –ø–æ –≤–∫–ª–∞–¥–∫–∞–º –±–µ–∑ location-info ‚Äî title –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å")
+
         # 2. –ö–ª–∏–∫–∞–µ–º –∏ —Å–∫—Ä–æ–ª–ª–∏–º –û—Ç–∑—ã–≤—ã (Reviews)
         try:
             reviews_tab = page.query_selector("div.tabs-select-view__title._name_reviews")
@@ -512,7 +695,7 @@ class YandexMapsInterceptionParser:
                 try:
                     photos_text = photos_tab.inner_text()
                     print(f"‚ÑπÔ∏è –¢–µ–∫—Å—Ç –≤–∫–ª–∞–¥–∫–∏ —Ñ–æ—Ç–æ: {photos_text}")
-                    match = re.search(r"(\\d+)", photos_text)
+                    match = re.search(r"(\d+)", photos_text)
                     if match:
                         extra_photos_count = int(match.group(1))
                 except Exception:
@@ -576,6 +759,24 @@ class YandexMapsInterceptionParser:
         except Exception as e:
             print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —É—Å–ª—É–≥: {e}")
 
+        # –°–±–æ—Ä ui_counts –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤–∫–ª–∞–¥–æ–∫ (—á—Ç–æ –≤–∏–¥–Ω–æ –Ω–∞ UI: 74 –æ—Ç–∑—ã–≤–∞, 64 —Ñ–æ—Ç–æ, 36 —É—Å–ª—É–≥)
+        try:
+            for sel, key in [
+                ("div.tabs-select-view__title._name_reviews", "reviews"),
+                ("div.tabs-select-view__title._name_gallery", "photos"),
+                ("div.tabs-select-view__title._name_price", "services"),
+            ]:
+                tab = page.query_selector(sel) or page.query_selector(
+                    "div.tabs-select-view__title._name_goods" if key == "services" else ""
+                )
+                if tab:
+                    txt = tab.inner_text()
+                    m = re.search(r"(\d+)", txt)
+                    if m:
+                        ui_counts[key] = int(m.group(1))
+        except Exception:
+            pass
+
         # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ HTML (—Ç–∞–∫ –∫–∞–∫ –≤ JSON —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ø—Ä—è—Ç–∞–Ω–æ)
         is_verified = False
         try:
@@ -601,9 +802,23 @@ class YandexMapsInterceptionParser:
 
         print(f"üì¶ –ü–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–æ {len(self.api_responses)} API –∑–∞–ø—Ä–æ—Å–æ–≤")
 
-        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
-        data = self._extract_data_from_responses()
+        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ (api_payload)
+        api_payload = self._extract_data_from_responses()
+        data = dict(api_payload)  # final –±—É–¥–µ—Ç –º–µ—Ä–¥–∂–∏—Ç—å—Å—è —Å html
         data["is_verified"] = is_verified
+
+        # –ñ—ë—Å—Ç–∫–∏–π fail: org API –Ω–µ –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω, –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π –Ω–µ—Ç ‚Äî –Ω–µ low_quality, –∞ org_api_not_loaded
+        has_org_api = any(
+            kw in u for u in self.api_responses
+            for kw in ("org", "location-info", "organization", "business", "company")
+        )
+        has_critical = bool(
+            data.get("address") or data.get("rating")
+            or (data.get("categories") and len(data.get("categories", [])) > 0)
+        )
+        if not has_org_api and not has_critical:
+            print("‚ùå Org API –Ω–µ –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω, –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π –Ω–µ—Ç. –í–æ–∑–≤—Ä–∞—â–∞–µ–º org_api_not_loaded.")
+            return {"error": "org_api_not_loaded", "url": page.url}
         if extra_photos_count > 0:
             data["photos_count"] = extra_photos_count
 
@@ -642,165 +857,92 @@ class YandexMapsInterceptionParser:
             except Exception as e:
                 print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Hybrid Mode –¥–ª—è —É—Å–ª—É–≥: {e}")
 
-        if not data.get("title") and not data.get("overview", {}).get("title"):
-            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ API, –∏—Å–ø–æ–ª—å–∑—É–µ–º HTML –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞–∫ fallback")
+        # HTML fallback: merge_non_empty ‚Äî –Ω–µ –∑–∞—Ç–∏—Ä–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ API-–ø–æ–ª—è –ø—É—Å—Ç—ã–º–∏
+        html_payload = self._extract_from_html_fallback(page)
+        meta_src_html = {k: "html_fallback" for k in html_payload if html_payload.get(k)}
+        merge_non_empty(data, html_payload, meta_src_html)
 
+        # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ user selector (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)
+        if not is_verified:
             try:
-                # 0. –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∏–∑ –º–µ—Ç–∞-—Ç–µ–≥–æ–≤ (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞)
-                meta_title = None
-                try:
-                    # og:title
-                    og_title = page.locator("meta[property='og:title']").get_attribute("content")
-                    if og_title:
-                        meta_title = og_title.split("|")[0].strip()  # "Name | City" -> "Name"
-                        print(f"‚úÖ –ù–∞—à–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ og:title: {meta_title}")
-
-                    # title tag
-                    if not meta_title:
-                        page_title = page.title()
-                        if page_title:
-                            meta_title = page_title.split("-")[0].strip()  # "Name - Yandex Maps" -> "Name"
-                            print(f"‚úÖ –ù–∞—à–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ page title: {meta_title}")
-                except Exception as e:
-                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞-–∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
-
-                # 0.1 –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ user selector (–µ—Å–ª–∏ –º–µ—Ç–∞ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞ –∏–ª–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
-                if not meta_title:
-                    try:
-                        h1_el = page.query_selector("div.orgpage-header-view__header-wrapper > h1")
-                        if h1_el:
-                            meta_title = h1_el.inner_text().strip()
-                            print(f"‚úÖ –ù–∞—à–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä: {meta_title}")
-                    except Exception as e:
-                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {e}")
-
-                if meta_title:
-                    if "overview" not in data:
-                        data["overview"] = {}
-                    data["title"] = meta_title
-                    data["overview"]["title"] = meta_title
+                verified_el = page.query_selector(
+                    "div.orgpage-header-view__header-wrapper > h1 > span.business-verified-badge, "
+                    "div.orgpage-header-view__header-wrapper > h1 > span"
+                )
+                if verified_el:
+                    data["is_verified"] = True
+                    print("‚úÖ –ù–∞–π–¥–µ–Ω–∞ –≥–∞–ª–æ—á–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ (User CSS)")
+            except Exception:
+                pass
 
-                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ user selector (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)
-                if not is_verified:
+        # –ü–µ—Ä–µ–¥–∞–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ø–∞—Ä—Å–µ—Ä (products HTML)
+        try:
+            if not data.get("products"):
+                print("üõ† Parsing services via HTML with USER Selectors...")
+                products_html: List[Dict[str, Any]] = []
+                groups = page.query_selector_all("div.business-full-items-grouped-view__content > div")
+                for group in groups:
+                    cat_title_el = group.query_selector("div.business-full-items-grouped-view__title")
+                    cat_title = cat_title_el.inner_text() if cat_title_el else "–î—Ä—É–≥–æ–µ"
+                    items = group.query_selector_all(
+                        "div.business-full-items-grouped-view__item, div.related-product-view"
+                    )
+                    if not items:
+                        items = group.query_selector_all("div.business-full-items-grouped-view__items._grid > div")
+                    for item in items:
+                        try:
+                            name_el = item.query_selector("div.related-product-view__title")
+                            price_el = item.query_selector("div.related-product-view__price")
+                            if name_el:
+                                products_html.append({
+                                    "name": name_el.inner_text(),
+                                    "price": price_el.inner_text() if price_el else "",
+                                    "category": cat_title,
+                                    "description": "",
+                                    "photo": "",
+                                })
+                        except Exception:
+                            pass
+                if not products_html:
                     try:
-                        # body > ... > h1 > span
-                        verified_el = page.query_selector(
-                            "div.orgpage-header-view__header-wrapper > h1 > span.business-verified-badge"
-                        )
-                        if not verified_el:
-                            verified_el = page.query_selector(
-                                "div.orgpage-header-view__header-wrapper > h1 > span"
-                            )
-
-                        if verified_el:
-                            data["is_verified"] = True
-                            print("‚úÖ –ù–∞–π–¥–µ–Ω–∞ –≥–∞–ª–æ—á–∫–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ (User CSS)")
-                    except Exception:
+                        from yandex_maps_scraper import parse_products
+                        products_html = parse_products(page)
+                    except ImportError:
                         pass
+                if products_html:
+                    print(f"‚úÖ HTML Fallback –Ω–∞—à–µ–ª {len(products_html)} —É—Å–ª—É–≥")
+                    current = data.get("products", [])
+                    current.extend(products_html)
+                    data["products"] = current
+        except Exception as e:
+            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ user-selector HTML parsing: {e}")
 
-                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ (–µ—Å–ª–∏ –Ω–µ—Ç –≤ API)
-                if not data.get("address") and not data.get("overview", {}).get("address"):
-                    try:
-                        # 1. Meta tag
-                        meta_address = page.locator(
-                            "meta[property='business:contact_data:street_address']"
-                        ).get_attribute("content")
-                        if meta_address:
-                            print(f"‚úÖ –ù–∞—à–ª–∏ –∞–¥—Ä–µ—Å –≤ meta: {meta_address}")
-                            data["address"] = meta_address
-                        else:
-                            # 2. CSS Selector
-                            address_el = (
-                                page.query_selector("div.orgpage-header-view__address")
-                                or page.query_selector("a.orgpage-header-view__address")
-                                or page.query_selector("div.business-contacts-view__address-link")
-                            )
-                            if address_el:
-                                addr_text = address_el.inner_text()
-                                print(f"‚úÖ –ù–∞—à–ª–∏ –∞–¥—Ä–µ—Å —á–µ—Ä–µ–∑ CSS: {addr_text}")
-                                data["address"] = addr_text
-                    except Exception as e:
-                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–¥—Ä–µ—Å–∞ HTML: {e}")
-
-            except Exception as e:
-                print(f"‚ö†Ô∏è Error extracting title from meta/css: {e}")
-
-            # –ü–µ—Ä–µ–¥–∞–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –ø–∞—Ä—Å–µ—Ä
-            try:
-                # –ü–æ—Å–∫–æ–ª—å–∫—É YandexMapsScraper –∫–ª–∞—Å—Å–∞ –Ω–µ—Ç, –ø–∞—Ä—Å–∏–º —Ä—É–∫–∞–º–∏
-
-                # Only try to parse products if we don't have them yet
-                if not data.get("products"):
-                    print("üõ† Parsing services via HTML with USER Selectors...")
-
-                    products_html: List[Dict[str, Any]] = []
-
-                    # 0. –°–Ω–∞—á–∞–ª–∞ –∫–ª–∏–∫–∞–µ–º –ø–æ —Ç–∞–±—É "–¶–µ–Ω—ã" –∏–ª–∏ "–£—Å–ª—É–≥–∏" –µ—Å–ª–∏ –µ—â–µ –Ω–µ —Ç–∞–º
-                    # (–í parse_yandex_card –º—ã —É–∂–µ –ø—Ä–æ–±–æ–≤–∞–ª–∏, –Ω–æ –º–æ–∂–µ—Ç –Ω–µ –≤—ã—à–ª–æ)
-                    # ...
-
-                    # 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–≥–∏–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Å–µ–ª–µ–∫—Ç–æ—Ä—ã)
-                    # Selector: body > ... > div.business-full-items-grouped-view__content
-
-                    groups = page.query_selector_all("div.business-full-items-grouped-view__content > div")
-                    for group in groups:
-                        # Category title?
-                        cat_title_el = group.query_selector("div.business-full-items-grouped-view__title")
-                        cat_title = cat_title_el.inner_text() if cat_title_el else "–î—Ä—É–≥–æ–µ"
-
-                        items = group.query_selector_all(
-                            "div.business-full-items-grouped-view__item, div.related-product-view"
-                        )
-                        if not items:
-                            # Try user selector
-                            items = group.query_selector_all(
-                                "div.business-full-items-grouped-view__items._grid > div"
-                            )
-
-                        for item in items:
-                            try:
-                                name_el = item.query_selector("div.related-product-view__title")
-                                price_el = item.query_selector("div.related-product-view__price")
-                                if name_el:
-                                    products_html.append(
-                                        {
-                                            "name": name_el.inner_text(),
-                                            "price": price_el.inner_text() if price_el else "",
-                                            "category": cat_title,
-                                            "description": "",
-                                            "photo": "",
-                                        }
-                                    )
-                            except Exception:
-                                pass
-
-                    # 2. –ï—Å–ª–∏ –Ω–µ –≤—ã—à–ª–æ - –ø—Ä–æ–±—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
-                    if not products_html:
-                        print("üîÑ –ü—Ä–æ–±—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é parse_products –∏–∑ yandex_maps_scraper...")
-                        try:
-                            from yandex_maps_scraper import parse_products
-
-                            products_html = parse_products(page)
-                        except ImportError:
-                            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å parse_products")
-
-                    if products_html:
-                        print(f"‚úÖ HTML Fallback –Ω–∞—à–µ–ª {len(products_html)} —É—Å–ª—É–≥")
-                        current = data.get("products", [])
-                        current.extend(products_html)
-                        data["products"] = current
-
-            except Exception as e:
-                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ user-selector HTML parsing: {e}")
+        # page_title –¥–ª—è fallback –≤ worker
+        try:
+            pt = page.title()
+            if pt and _is_empty(data.get("page_title")):
+                data["page_title"] = pt
+        except Exception:
+            pass
 
-            # –ü—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑ –ø–æ–ª—É—á–∏—Ç—å title –µ—Å–ª–∏ –Ω–µ—Ç
-            if not data.get("title"):
-                try:
-                    title_el = page.query_selector("h1.orgpage-header-view__header")
-                    if title_el:
-                        data["title"] = title_el.inner_text()
-                except Exception:
-                    pass
+        # Warnings –ø–æ –ø–æ–ª–Ω–æ—Ç–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (ui_counts vs extracted)
+        extracted_reviews = len(data.get("reviews") or [])
+        extracted_photos = data.get("photos_count") or len(data.get("photos") or [])
+        products_raw = data.get("products") or []
+        extracted_services = sum(len(g.get("items", [])) for g in products_raw if isinstance(g, dict)) if products_raw else 0
+        if not extracted_services and products_raw:
+            extracted_services = len(products_raw)
+        warnings_list: List[str] = list(data.get("warnings") or [])
+        if ui_counts.get("reviews", 0) >= 50 and extracted_reviews < ui_counts["reviews"] * 0.8:
+            warnings_list.append("reviews_incomplete")
+        if ui_counts.get("photos", 0) > 0 and extracted_photos == 0:
+            warnings_list.append("photos_not_extracted")
+        if ui_counts.get("services", 0) > 0 and extracted_services < ui_counts["services"] * 0.8:
+            warnings_list.append("services_incomplete")
+        data["warnings"] = warnings_list
+
+        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è title_or_name –ü–ï–†–ï–î return (–∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç)
+        _normalize_title_fields(data)
 
         # DEBUG BUNDLE (dev): —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–≤–æ–¥–∫—É –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∏ –ø–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ bundle)
         if self.debug_bundle_dir:
@@ -856,12 +998,9 @@ class YandexMapsInterceptionParser:
                 except Exception:
                     pass
 
-                # –ü—Ä–∏–∑–Ω–∞–∫–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ / –∫–∞–ø—á–∏
+                # –ü—Ä–∏–∑–Ω–∞–∫–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ / –∫–∞–ø—á–∏ (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ, —Ä—É—Å/–∞–Ω–≥–ª)
                 blocked_flags = {
-                    "has_captcha_text": any(
-                        kw in (page_title or "")
-                        for kw in ["–û–π!", "Captcha", "Robot", "–í—ã –Ω–µ —Ä–æ–±–æ—Ç"]
-                    ),
+                    "has_captcha_text": _is_captcha_page(page_title or ""),
                     "html_contains_captcha": ("smart-captcha" in (html_content or "")),
                 }
 
@@ -999,15 +1138,83 @@ class YandexMapsInterceptionParser:
                 except Exception:
                     pass
 
-                # –°—ã—Ä–æ–π payload (–∏—Ç–æ–≥–æ–≤—ã–π card_data)
+                # api_extract.json ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç extractor'–∞ –∏–∑ API (–¥–æ HTML merge)
+                try:
+                    api_extract = {
+                        "title": api_payload.get("title"),
+                        "title_or_name": api_payload.get("title_or_name"),
+                        "address": api_payload.get("address"),
+                        "categories": api_payload.get("categories"),
+                        "rating": api_payload.get("rating"),
+                        "reviews_count": api_payload.get("reviews_count"),
+                    }
+                    # location-info structure helper: top-level keys –∏ 1‚Äì2 —É—Ä–æ–≤–Ω—è
+                    li_struct = None
+                    if self.location_info_payload:
+                        li = self.location_info_payload
+                        li_keys = list(li.keys())[:20] if isinstance(li, dict) else []
+                        li_nested = {}
+                        if isinstance(li, dict) and "data" in li:
+                            d = li["data"]
+                            li_nested["data_keys"] = list(d.keys())[:25] if isinstance(d, dict) else []
+                            if isinstance(d, dict) and "toponymSearchResult" in d:
+                                tsr = d["toponymSearchResult"]
+                                li_nested["toponymSearchResult_keys"] = list(tsr.keys())[:15] if isinstance(tsr, dict) else []
+                        li_struct = {"top_keys": li_keys, "nested": li_nested}
+                    api_extract["location_info_structure"] = li_struct
+                    with open(os.path.join(bundle_dir, "api_extract.json"), "w", encoding="utf-8") as f:
+                        json.dump(api_extract, f, ensure_ascii=False, indent=2)
+                except Exception as e:
+                    print(f"‚ö†Ô∏è Failed to write api_extract.json: {e}")
+
+                # payload.json (legacy) –∏ final_payload.json ‚Äî —Ç–æ, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∏–∑ parse_yandex_card
                 try:
                     with open(os.path.join(bundle_dir, "payload.json"), "w", encoding="utf-8") as f:
                         json.dump(data, f, ensure_ascii=False, indent=2)
+                    with open(os.path.join(bundle_dir, "final_payload.json"), "w", encoding="utf-8") as f:
+                        json.dump(data, f, ensure_ascii=False, indent=2)
                 except Exception as e:
-                    print(f"‚ö†Ô∏è Failed to write payload.json: {e}")
+                    print(f"‚ö†Ô∏è Failed to write payload.json/final_payload.json: {e}")
+
+                # selected_item_debug.json ‚Äî search API: —Å–∫–æ–ª—å–∫–æ items, –∫–∞–∫–æ–π –≤—ã–±—Ä–∞–Ω, –ø–æ –∫–∞–∫–æ–º—É –∫–ª—é—á—É
+                try:
+                    if getattr(self, "_search_api_debug", None):
+                        with open(os.path.join(bundle_dir, "selected_item_debug.json"), "w", encoding="utf-8") as f:
+                            json.dump(self._search_api_debug, f, ensure_ascii=False, indent=2)
+                except Exception as e:
+                    print(f"‚ö†Ô∏è Failed to write selected_item_debug.json: {e}")
+
+                # –°—á—ë—Ç—á–∏–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å UI
+                try:
+                    services_list = data.get("products") or []
+                    if isinstance(services_list, list):
+                        services_count = sum(len(g.get("items", [])) for g in services_list if isinstance(g, dict)) or len(services_list)
+                    else:
+                        services_count = 0
+                    counts = {
+                        "extracted": {
+                            "services": services_count,
+                            "reviews": len(data.get("reviews") or []),
+                            "photos": data.get("photos_count") or len(data.get("photos") or []),
+                        },
+                        "raw_before_dedup": {
+                            "services": getattr(self, "_debug_raw_services_count", 0),
+                        },
+                    }
+                    with open(os.path.join(bundle_dir, "counts_comparison.json"), "w", encoding="utf-8") as f:
+                        json.dump(counts, f, ensure_ascii=False, indent=2)
+                except Exception as ce:
+                    print(f"‚ö†Ô∏è Failed to write counts_comparison.json: {ce}")
             except Exception as e:
                 print(f"‚ö†Ô∏è Failed to write canonical debug bundle files: {e}")
 
+        # [PHOTOS_COUNT]
+        photos_count = data.get("photos_count") or len(data.get("photos") or [])
+        print(f"[PHOTOS_COUNT] extracted={photos_count}")
+
+        # DATA_TRACE
+        print(f"[DATA_TRACE] data.title: {data.get('title')}, title_or_name: {data.get('title_or_name')}")
+
         print(
             f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ: –Ω–∞–∑–≤–∞–Ω–∏–µ='{data.get('title', '')}', –∞–¥—Ä–µ—Å='{data.get('address', '')}'"
         )
@@ -1052,14 +1259,23 @@ class YandexMapsInterceptionParser:
                     data['reviews'] = reviews
                     data['reviews_count'] = len(reviews)
             
+            # Search API —Å business_oid ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ title/address –¥–ª—è org-—Å—Ç—Ä–∞–Ω–∏—Ü
+            elif 'search' in url and 'business_oid' in url:
+                search_data = self._extract_search_api_business(json_data)
+                if search_data:
+                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∏–∑ search API (business_oid)")
+                    _search_debug = search_data.pop("_search_debug", None)
+                    if _search_debug is not None:
+                        self._search_api_debug = _search_debug
+                    meta_src = search_data.pop("meta", None) or {}
+                    merge_non_empty(data, search_data, meta_src)
             # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è location-info API
             elif 'location-info' in url:
                 org_data = self._extract_location_info(json_data)
                 if org_data:
                     print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ location-info API")
-                if org_data:
-                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ location-info API")
-                    data.update(org_data)
+                    meta_src = {k: "location_info" for k in org_data if k not in ("meta",) and org_data.get(k)}
+                    merge_non_empty(data, org_data, meta_src)
             
             # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è fetchGoods/Prices API
             elif 'fetchGoods' in url or 'prices' in url.lower() or 'goods' in url.lower() or 'product' in url.lower() or 'search' in url.lower() or 'catalog' in url.lower():
@@ -1108,7 +1324,10 @@ class YandexMapsInterceptionParser:
                     pass
         
         # Deduplicate products by name and price
+        raw_services_count = 0
         if data.get('products'):
+            raw_services_count = len(data['products'])
+            self._debug_raw_services_count = raw_services_count
             unique_products = {}
             for p in data['products']:
                 # Key: Name + Price (to distinguish "Haircut" 500 vs "Haircut" 1000)
@@ -1117,7 +1336,11 @@ class YandexMapsInterceptionParser:
                 if key not in unique_products:
                     unique_products[key] = p
             data['products'] = list(unique_products.values())
-            print(f"‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—Å–ª—É–≥ –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(data['products'])}")
+            unique_count = len(data['products'])
+            print(f"‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —É—Å–ª—É–≥ –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {unique_count}")
+            print(f"[SERVICES_COUNT] raw_extracted={raw_services_count}, after_dedup={unique_count}")
+            if raw_services_count != unique_count:
+                print(f"[SERVICES_DUPLICATES] found {raw_services_count - unique_count} duplicates")
         
         # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Ç—á–µ—Ç–æ–º)
         if data.get('products'):
@@ -1147,6 +1370,11 @@ class YandexMapsInterceptionParser:
         ]
         data['overview'] = {k: data.get(k, '') for k in overview_keys}
         data['overview']['reviews_count'] = data.get('reviews_count', 0)
+
+        reviews_count = len(data.get('reviews', []))
+        print(f"[REVIEWS_COUNT] extracted={reviews_count}")
+        if reviews_count < 50:
+            print(f"[REVIEWS_MISSING] possible pagination or filtering issue (count={reviews_count})")
         
         return data
     
@@ -1231,10 +1459,189 @@ class YandexMapsInterceptionParser:
         extract_nested(json_data)
         return result
     
+    def _extract_from_html_fallback(self, page) -> Dict[str, Any]:
+        """
+        –ò–∑–≤–ª–µ–∫–∞–µ—Ç title/address –∏–∑ HTML (og:title, page title, h1).
+        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict ‚Äî —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–µ—Ä–¥–∂–∏—Ç—Å—è –≤ final, —Ç–æ–ª—å–∫–æ –∑–∞–ø–æ–ª–Ω—è—è –ø—É—Å—Ç—ã–µ —Å–ª–æ—Ç—ã.
+        """
+        out: Dict[str, Any] = {}
+        try:
+            og_title = page.locator("meta[property='og:title']").get_attribute("content")
+            if og_title:
+                out["og_title"] = og_title
+                clean = og_title.replace(" ‚Äî –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "").replace(" - –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "").split("|")[0].split(",")[0].strip()
+                t = clean or og_title.split("|")[0].strip()
+                if t:
+                    out["title_or_name"] = t
+                    out["title"] = t
+                    print(f"‚úÖ [HTML_FALLBACK] og:title -> {t[:50]}")
+            if not out.get("title"):
+                pt = page.title()
+                if pt:
+                    out["page_title"] = pt
+                    t = pt.replace(" ‚Äî –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "").replace(" - –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "").split("|")[0].split("-")[0].strip()
+                    if t:
+                        out["title_or_name"] = t
+                        out["title"] = t
+                        print(f"‚úÖ [HTML_FALLBACK] page_title -> {t[:50]}")
+            if not out.get("title"):
+                h1 = page.query_selector("div.orgpage-header-view__header-wrapper > h1, h1.orgpage-header-view__header")
+                if h1:
+                    t = h1.inner_text().strip()
+                    if t:
+                        out["title_or_name"] = t
+                        out["title"] = t
+                        print(f"‚úÖ [HTML_FALLBACK] h1 -> {t[:50]}")
+            # –ê–¥—Ä–µ—Å
+            meta_addr = page.locator("meta[property='business:contact_data:street_address']").get_attribute("content")
+            if meta_addr:
+                out["address"] = meta_addr
+            elif not out.get("address"):
+                addr_el = page.query_selector("div.orgpage-header-view__address, a.orgpage-header-view__address, div.business-contacts-view__address-link")
+                if addr_el:
+                    out["address"] = addr_el.inner_text().strip()
+        except Exception as e:
+            print(f"‚ö†Ô∏è [HTML_FALLBACK] error: {e}")
+        return out
+
+    def _extract_search_api_business(self, json_data: Any) -> Dict[str, Any]:
+        """
+        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –±–∏–∑–Ω–µ—Å–∞ –∏–∑ search API (business_oid=...).
+        –í—ã–±–∏—Ä–∞–µ—Ç item, –≥–¥–µ id/oid/businessOid —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å self.org_id.
+        –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π result (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º search API).
+        """
+        result: Dict[str, Any] = {}
+        expected_oid = self.org_id
+        items = _pick_first(json_data, [["data", "items"], ["items"]])
+        if not isinstance(items, list) or not items:
+            return result
+
+        # –ù–∞–π—Ç–∏ item —Å —Å–æ–≤–ø–∞–¥–∞—é—â–∏–º org_id (id, oid, businessOid, uri oid=)
+        def _item_oid(it: Any) -> Optional[str]:
+            if not isinstance(it, dict):
+                return None
+            oid = it.get("id") or it.get("oid") or it.get("businessOid")
+            if oid is not None:
+                return str(oid)
+            uri = it.get("uri") or ""
+            if isinstance(uri, str) and "oid=" in uri:
+                m = re.search(r"oid=(\d+)", uri)
+                if m:
+                    return m.group(1)
+            return None
+
+        item = None
+        matched_key = None
+        selected_index = -1
+        for i, it in enumerate(items):
+            oid = _item_oid(it)
+            if oid and expected_oid and str(oid) == str(expected_oid):
+                item = it
+                selected_index = i
+                if it.get("id") is not None and str(it.get("id")) == str(expected_oid):
+                    matched_key = "id"
+                elif it.get("oid") is not None and str(it.get("oid")) == str(expected_oid):
+                    matched_key = "oid"
+                elif it.get("businessOid") is not None and str(it.get("businessOid")) == str(expected_oid):
+                    matched_key = "businessOid"
+                else:
+                    matched_key = "uri"
+                break
+
+        if not item or not isinstance(item, dict):
+            return result
+
+        # meta –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ–ª–µ–π
+        meta_src: Dict[str, str] = {}
+        BLACKLIST = ("–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–†–æ—Å—Å–∏—è", "–Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "–ú–æ—Å–∫–≤–∞")
+        title = item.get("title") or item.get("shortTitle") or item.get("name")
+        if title and str(title).strip() not in BLACKLIST:
+            result["title"] = str(title).strip()
+            result["title_or_name"] = result["title"]
+            meta_src["title"] = meta_src["title_or_name"] = "search_api"
+        addr = item.get("fullAddress") or item.get("address")
+        if addr and isinstance(addr, str) and len(addr.strip()) > 2:
+            result["address"] = addr.strip()
+            meta_src["address"] = "search_api"
+        if item.get("categories"):
+            cats = []
+            for c in item["categories"]:
+                if isinstance(c, dict) and c.get("name"):
+                    cats.append(str(c["name"]))
+                elif isinstance(c, str):
+                    cats.append(c)
+            if cats:
+                result["categories"] = cats
+                meta_src["categories"] = "search_api"
+        rd = item.get("ratingData")
+        if isinstance(rd, dict):
+            rv = rd.get("ratingValue") or rd.get("rating") or rd.get("value")
+            if rv is not None:
+                result["rating"] = str(rv)
+                meta_src["rating"] = "search_api"
+            rc = rd.get("reviewCount") or rd.get("reviewsCount") or rd.get("count")
+            if rc is not None:
+                try:
+                    result["reviews_count"] = int(rc)
+                    meta_src["reviews_count"] = "search_api"
+                except (TypeError, ValueError):
+                    pass
+        result["meta"] = meta_src
+        result["_search_debug"] = {
+            "items_count": len(items),
+            "selected_index": selected_index,
+            "matched_key": matched_key,
+            "expected_oid": expected_oid,
+        }
+        return result
+
     def _extract_location_info(self, json_data: Any) -> Dict[str, Any]:
-        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ location-info API"""
+        """
+        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ location-info API.
+        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç _pick_first —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –∏–∑ JSON.
+        location-info: data.toponymSearchResult.items[0].title (–≥–æ—Ä–æ–¥), –±–∏–∑–Ω–µ—Å –≤ showcase.
+        search API ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–ª—è org-—Å—Ç—Ä–∞–Ω–∏—Ü.
+        """
         result = {}
-        
+        BLACKLIST = ("–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥", "–†–æ—Å—Å–∏—è", "–Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "–ú–æ—Å–∫–≤–∞")
+
+        # –ü—Ä—è–º—ã–µ –ø—É—Ç–∏ –∏–∑ location-info (–ø–æ —Ä–µ–∞–ª—å–Ω—ã–º bundle)
+        title = _pick_first(json_data, [
+            ["data", "toponymSearchResult", "items", "0", "toponymDiscovery", "categories", "0", "showcase", "0", "title"],
+            ["data", "toponymSearchResult", "exactResult", "toponymDiscovery", "categories", "0", "showcase", "0", "title"],
+            ["data", "toponymSearchResult", "items", "0", "title"],
+            ["data", "toponymSearchResult", "exactResult", "title"],
+        ])
+        if title and str(title).strip() not in BLACKLIST:
+            result["title"] = str(title).strip()
+            result["title_or_name"] = result["title"]
+
+        addr = _pick_first(json_data, [
+            ["data", "toponymSearchResult", "items", "0", "toponymDiscovery", "categories", "0", "showcase", "0", "fullAddress"],
+            ["data", "toponymSearchResult", "items", "0", "toponymDiscovery", "categories", "0", "showcase", "0", "address"],
+            ["data", "toponymSearchResult", "items", "0", "fullAddress"],
+            ["data", "toponymSearchResult", "items", "0", "address"],
+            ["data", "toponymSearchResult", "exactResult", "address"],
+        ])
+        if addr and isinstance(addr, str) and len(addr.strip()) > 2 and addr.strip() not in BLACKLIST:
+            result["address"] = addr.strip()
+
+        # ratingData –∏–∑ showcase
+        rd = _pick_first(json_data, [
+            ["data", "toponymSearchResult", "items", "0", "toponymDiscovery", "categories", "0", "showcase", "0", "ratingData"],
+            ["data", "toponymSearchResult", "items", "0", "ratingData"],
+        ])
+        if isinstance(rd, dict):
+            rv = rd.get("ratingValue") or rd.get("rating") or rd.get("value")
+            if rv is not None:
+                result["rating"] = str(rv)
+            rc = rd.get("reviewCount") or rd.get("reviewsCount") or rd.get("count")
+            if rc is not None:
+                try:
+                    result["reviews_count"] = int(rc)
+                except (TypeError, ValueError):
+                    pass
+
         def extract_nested(data):
             if isinstance(data, dict):
                 # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
diff --git a/src/worker.py b/src/worker.py
index addd3f3..09dfe11 100644
--- a/src/worker.py
+++ b/src/worker.py
@@ -184,11 +184,6 @@ def _parse_date_string(date_str: str) -> datetime | None:
     if russian_date:
         return russian_date
     
-    # –ü—Ä–æ–±—É–µ–º —Ä—É—Å—Å–∫–∏–µ –¥–∞—Ç—ã (27 —è–Ω–≤–∞—Ä—è 2026)
-    russian_date = _parse_russian_date(date_str)
-    if russian_date:
-        return russian_date
-    
     # –ü—Ä–æ–±—É–µ–º ISO —Ñ–æ—Ä–º–∞—Ç
     try:
         if 'T' in date_str or 'Z' in date_str or date_str.count('-') >= 2:
@@ -198,7 +193,6 @@ def _parse_date_string(date_str: str) -> datetime | None:
     
     # –ü—Ä–æ–±—É–µ–º dateutil –¥–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
     try:
-        from dateutil import parser as date_parser
         return date_parser.parse(date_str, fuzzy=True)
     except Exception:
         return None
@@ -660,6 +654,40 @@ def process_queue():
                 debug_bundle_id=debug_bundle_id,
                 **geolocation_kwarg,
             )
+
+            # –ó–∞—â–∏—Ç–∞ –æ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤–æ–∑–≤—Ä–∞—Ç–∞ –ø–∞—Ä—Å–µ—Ä–∞
+            if card_data is None:
+                print("[FATAL] parse_yandex_card –≤–µ—Ä–Ω—É–ª None", flush=True)
+                card_data = {"error": "parser_returned_none", "url": url}
+            elif not isinstance(card_data, dict):
+                print(f"[FATAL] parse_yandex_card –≤–µ—Ä–Ω—É–ª {type(card_data)}", flush=True)
+                card_data = {"error": f"parser_returned_{type(card_data).__name__}", "url": url}
+
+            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è title_or_name –¥–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–∫–∞—Å–∫–∞–¥: title ‚Üí name ‚Üí overview ‚Üí page_title ‚Üí og_title)
+            if isinstance(card_data, dict) and not card_data.get("error"):
+                if not card_data.get("title_or_name", "").strip():
+                    og_raw = (card_data.get("og_title") or "").strip()
+                    og_clean = og_raw.replace(" ‚Äî –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "").replace(" - –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "").split("|")[0].split(",")[0].strip() if og_raw else ""
+                    sources = [
+                        (card_data.get("title") or "").strip(),
+                        (card_data.get("name") or "").strip(),
+                        (card_data.get("overview") or {}).get("title") if isinstance(card_data.get("overview"), dict) else "",
+                        (card_data.get("page_title") or "").replace(" ‚Äî –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "").replace(" - –Ø–Ω–¥–µ–∫—Å –ö–∞—Ä—Ç—ã", "").strip() if card_data.get("page_title") else "",
+                        og_clean,
+                    ]
+                    fallback = next((s for s in sources if s and str(s).strip()), None)
+                    if fallback:
+                        fallback = str(fallback).strip()
+                        card_data["title_or_name"] = fallback
+                        if not card_data.get("title"):
+                            card_data["title"] = fallback
+                        overview = card_data.get("overview") or {}
+                        if isinstance(overview, dict) and not overview.get("title"):
+                            overview["title"] = fallback
+                        used_og = fallback == og_clean and og_clean
+                        print(f"[WORKER_NORMALIZE] title_or_name='{fallback[:50]}'" + (" (from og_title)" if used_og else " –∏–∑ title/name/overview/page_title"), flush=True)
+                    else:
+                        print("[CRITICAL] –ù–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è title_or_name", flush=True)
         except Exception as e:
             msg = str(e)
             if "Playwright Sync API inside the asyncio loop" in msg:
@@ -711,6 +739,9 @@ def process_queue():
         if bundle_dir and validation_result:
             try:
                 v_path = os.path.join(bundle_dir, "validation.json")
+                val_warnings = list(validation_result.get("warnings") or [])
+                parser_warnings = list(card_data.get("warnings") or []) if isinstance(card_data, dict) else []
+                all_warnings = list(dict.fromkeys(val_warnings + parser_warnings))
                 payload = {
                     "is_successful": bool(is_successful),
                     "reason": str(reason),
@@ -718,7 +749,7 @@ def process_queue():
                     "hard_missing": validation_result.get("hard_missing") or [],
                     "missing_fields": validation_result.get("missing_fields") or [],
                     "found_fields": validation_result.get("found_fields") or [],
-                    "warnings": validation_result.get("warnings") or [],
+                    "warnings": all_warnings,
                 }
                 with open(v_path, "w", encoding="utf-8") as f:
                     json.dump(payload, f, ensure_ascii=False, indent=2, default=str)
@@ -741,39 +772,7 @@ def process_queue():
             meta = build_parsing_meta(card_data, validation_result, source=SOURCE_YANDEX_BUSINESS)
             card_data["_meta"] = meta
         
-        fallback_created = False
         if not is_successful and business_id:
-            # DISABLE AUTOMATIC FALLBACK (User Request 2026-01-23)
-            # Fallback to cabinet parsing should be manual only.
-            # has_account, account_id = _has_cabinet_account(business_id)
-            # if has_account: ...
-            
-            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–∞–±–∏–Ω–µ—Ç –¥–ª—è fallback
-            # has_account, account_id = _has_cabinet_account(business_id)
-            
-            # if has_account:
-            #     print(f"‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ–ø–æ–ª–Ω—ã–π ({reason}), —Å–æ–∑–¥–∞—é –∑–∞–¥–∞—á—É fallback —á–µ—Ä–µ–∑ –∫–∞–±–∏–Ω–µ—Ç")
-                
-            #     # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É fallback
-            #     fallback_task_id = str(uuid.uuid4())
-            #     conn = get_db_connection()
-            #     cursor = conn.cursor()
-                
-            #     try:
-            #         cursor.execute("""
-            #             INSERT INTO ParseQueue (
-            #                 id, business_id, account_id, task_type, source,
-            #                 status, user_id, url, created_at, updated_at
-            #             )
-            #             VALUES (?, ?, ?, 'parse_cabinet_fallback', 'yandex_business',
-            #                     'pending', ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
-            #         """, (fallback_task_id, business_id, account_id, queue_dict["user_id"], queue_dict["url"]))
-            #         conn.commit()
-            #         print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∑–∞–¥–∞—á–∞ fallback: {fallback_task_id}")
-            #         fallback_created = True
-            #     finally:
-            #         cursor.close()
-            #         conn.close()
             print(f"‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ–ø–æ–ª–Ω—ã–π ({reason}). –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fallback –æ—Ç–∫–ª—é—á–µ–Ω.")
         
         if card_data.get("error") == "captcha_detected":
@@ -783,26 +782,6 @@ def process_queue():
             else:
                 print("‚ö†Ô∏è –ö–∞–ø—á–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –Ω–æ session_id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (registry –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
 
-            # –ï—Å–ª–∏ –±—ã–ª —Å–æ–∑–¥–∞–Ω —Ñ–æ–ª–ª–±—ç–∫, —Ç–æ —Å—á–∏—Ç–∞–µ–º –∑–∞–¥–∞—á—É –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π, –Ω–µ —É—Ö–æ–¥–∏–º –≤ —Ü–∏–∫–ª
-            if fallback_created:
-                print(
-                    "‚úÖ –ö–∞–ø—á–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –Ω–æ —Å–æ–∑–¥–∞–Ω —Ñ–æ–ª–ª–±—ç–∫. "
-                    "–ü–æ–º–µ—á–∞—é –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ü–∏–∫–ª–∏–≤–∞—Ç—å."
-                )
-                conn = get_db_connection()
-                cursor = conn.cursor()
-                try:
-                    cursor.execute(
-                        "UPDATE parsequeue SET status = %s, updated_at = CURRENT_TIMESTAMP WHERE id = %s",
-                        (STATUS_COMPLETED, queue_dict["id"]),
-                    )
-                    cursor.execute("DELETE FROM parsequeue WHERE id = %s", (queue_dict["id"],))
-                    conn.commit()
-                finally:
-                    cursor.close()
-                    conn.close()
-                return
-
             # –û—Ç–∫—Ä—ã–≤–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∫–∞–ø—á–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
             conn = get_db_connection()
             cursor = conn.cursor()
@@ -1042,6 +1021,14 @@ def process_queue():
                                 from safe_db_utils import get_db_path
                                 db_path_debug = get_db_path()
                                 r_len = len(reviews_list) if reviews_list else 0
+                                # –†–∞—Å—á—ë—Ç –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤ (–µ—Å–ª–∏ reviews_list –æ–ø—Ä–µ–¥–µ–ª—ë–Ω)
+                                if 'reviews_list' in locals() and reviews_list:
+                                    unanswered_reviews_count = sum(
+                                        1 for r in reviews_list
+                                        if not r.get("org_reply")
+                                    )
+                                else:
+                                    unanswered_reviews_count = 0
                                 debug_log(f"Worker DB Path: {db_path_debug}")
                                 debug_log(f"Reviews in list: {r_len}")
                                 debug_log(f"Unanswered calc: {unanswered_reviews_count}")
@@ -1320,25 +1307,6 @@ def process_queue():
                 except Exception as analysis_error:
                     print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ò–ò-–∞–Ω–∞–ª–∏–∑–µ –∫–∞—Ä—Ç–æ—á–∫–∏ {card_id}: {analysis_error}")
             
-            # --- SYNC SERVICES AFTER PARSING (NEW) ---
-            if business_id and card_data.get('products'):
-                try:
-                    # Need owner_id for sync
-                    cursor = conn.cursor() # Ensure we have cursor
-                    cursor.execute("SELECT owner_id FROM businesses WHERE id = %s", (business_id,))
-                    owner_row = cursor.fetchone()
-                    if owner_row:
-                        owner_id = owner_row[0] if isinstance(owner_row, (list, tuple)) else owner_row.get("owner_id")
-                        print(f"üîÑ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —É—Å–ª—É–≥ –¥–ª—è business_id={business_id} (owner_id={owner_id})...")
-                        _sync_parsed_services_to_db(business_id, card_data.get('products'), conn, owner_id)
-                        print(f"‚úÖ –£—Å–ª—É–≥–∏ —É—Å–ø–µ—à–Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")
-                    else:
-                        print(f"‚ö†Ô∏è Cannot sync services: owner_id not found for business {business_id}")
-                except Exception as sync_error:
-                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —É—Å–ª—É–≥: {sync_error}")
-                    import traceback
-                    traceback.print_exc()
-            
             # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ "completed" (—á—Ç–æ–±—ã –∑–∞–¥–∞—á–∞ –æ—Å—Ç–∞–ª–∞—Å—å –≤ —Å–ø–∏—Å–∫–µ)
             warning_parts = []
             # –°—Ç–∞—Ä–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–æ HTML fallback (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±—ã—Å—Ç—Ä—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç)
@@ -1431,22 +1399,30 @@ def map_card_services(card_data: Dict[str, Any], business_id: str, user_id: str)
         return []
     rows = []
     source = "yandex_maps"
-    # –§–æ—Ä–º–∞—Ç: [{"category": "‚Ä¶", "items": [{"name", "price", "description", "id", ...}]}] –∏–ª–∏ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫
     for cat_block in products:
+        # cat_block –º–æ–∂–µ—Ç –±—ã—Ç—å: dict (–∫–∞—Ç–µ–≥–æ—Ä–∏—è), list (–≤–ª–æ–∂–µ–Ω–Ω—ã–µ items), –∏–ª–∏ –º—É—Å–æ—Ä
+        if isinstance(cat_block, list):
+            # –ü–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ items –±–µ–∑ –æ–±—ë—Ä—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
+            items = cat_block
+            category_name = "–†–∞–∑–Ω–æ–µ"
+            for item in items:
+                if isinstance(item, dict) and item.get("name"):
+                    row = _one_service_row(item, business_id, user_id, source)
+                    if row:
+                        row["category"] = category_name
+                        rows.append(row)
+            continue
+
         if not isinstance(cat_block, dict):
-            if isinstance(cat_block, (str, int, float)):
-                continue
-            if isinstance(cat_block, list):
-                for item in cat_block:
-                    if isinstance(item, dict) and item.get("name"):
-                        row = _one_service_row(item, business_id, user_id, source)
-                        if row:
-                            rows.append(row)
+            # –ü—Ä–∏–º–∏—Ç–∏–≤—ã –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–∏–ø—ã ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
             continue
+
+        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: dict —Å category –∏ items
         category_name = (cat_block.get("category") or "–†–∞–∑–Ω–æ–µ").strip() or "–†–∞–∑–Ω–æ–µ"
         items = cat_block.get("items") or cat_block.get("products") or []
         if not isinstance(items, list):
             continue
+
         for item in items:
             if not isinstance(item, dict) or not item.get("name"):
                 continue
@@ -1469,11 +1445,18 @@ def _one_service_row(item: Dict[str, Any], business_id: str, user_id: str, sourc
     price_from = price_to = None
     if raw_price is not None and str(raw_price).strip():
         try:
-            digits = re.sub(r"[^0-9.,]", "", str(raw_price))
-            digits = digits.replace(",", ".")
-            if digits:
-                price_from = float(digits)
-                price_to = price_from
+            # –ò—â–µ–º –≤—Å–µ —á–∏—Å–ª–∞ –≤ —Å—Ç—Ä–æ–∫–µ
+            numbers = re.findall(r"\d+", str(raw_price).replace(" ", "").replace(",", ""))
+            if len(numbers) >= 2:
+                n1, n2 = int(numbers[0]), int(numbers[1])
+                # –ó–∞—â–∏—Ç–∞ –æ—Ç –≤—ã–±—Ä–æ—Å–æ–≤: –µ—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ >100x, –≤–µ—Ä–æ—è—Ç–Ω–æ —ç—Ç–æ –Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω
+                if max(n1, n2) / max(min(n1, n2), 1) <= 100:
+                    price_from = float(min(n1, n2))
+                    price_to = float(max(n1, n2))
+                else:
+                    price_from = price_to = float(n1)
+            elif len(numbers) == 1:
+                price_from = price_to = float(numbers[0])
         except (ValueError, TypeError):
             pass
     return {
@@ -1486,7 +1469,7 @@ def _one_service_row(item: Dict[str, Any], business_id: str, user_id: str, sourc
         "external_id": external_id,
         "price_from": price_from,
         "price_to": price_to,
-        "raw": item,
+        "raw": (dict(item) if isinstance(item, dict) else {"_error": "not_a_dict", "_type": type(item).__name__}),
         "duration_minutes": item.get("duration_minutes") or item.get("duration"),
     }
 
